#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Sun Aug 18 12:42:26 2019
@author: divyarth
"""

import numpy as np
import pandas as pd
from sklearn.preprocessing import LabelEncoder
import sys

labels=['bias','0_usual', '0_pretentious', '0_great_pret', 
'1_proper', '1_less_proper', '1_improper', '1_critical', '1_very_crit', 
'2_complete', '2_completed', '2_incomplete', '2_foster', 
'3_1', '3_2', '3_3', '3_more', 
'4_convenient', '4_less_conv', '4_critical', 
'5_convenient', '5_inconv', 
'6_nonprob', '6_slightly_prob', '6_problematic', 
'7_recommended', '7_priority', '7_not_recom']

data = pd.read_csv(sys.argv[1],header=None)
data.columns=['0','1','2','3','4','5','6','7','response']
data['bias']=pd.Series(np.asarray(len(data)*[1])) #'8' is the bias
X_temp=data[['0','1','2','3','4','5','6','7','bias']]
y_temp=data['response']

le = LabelEncoder()

le.fit(y_temp)
y=le.transform(y_temp)
X=pd.get_dummies(X_temp)
X=X[labels]

num_ftr = len(X.columns)
num_dat=len(data)
num_res=len(le.classes_)  # added one for bias
w=np.zeros((num_ftr,num_res))
w0=w
data_test = pd.read_csv(sys.argv[2],header=None)


data_test.columns=['0','1','2','3','4','5','6','7']
data_test['bias']=pd.Series(np.asarray(len(data_test)*[1]))
X_temp_test=data_test[['0','1','2','3','4','5','6','7','bias']]
X_test=pd.get_dummies(X_temp_test)
X_test=X_test[labels]

def softmax(w,X):
    return np.exp(X.dot(w)).mul(1/np.sum(np.exp(X.dot(w)),axis=1),axis=0) #pehle nhi chal ra tha array divide krra tha, yeh efficient hai

def loss(y_hat,y):    
    alpha=pd.DataFrame(pd.get_dummies(y))
    return -np.log(y_hat[alpha!=0].sum(axis=1)).sum()/len(y)

def get_loss(w,X,y):
    y_hat=softmax(w,X)
    return loss(y_hat,y)

def gradDes(X,y,w0,eta,iter0):
    loss=get_loss(w0,X,y)
    p=softmax(w0,X)
    w=w0
    alpha=np.asarray(pd.get_dummies(y))
    for i in range(iter0):
        w0=w
        p=softmax(w,X)
        der=X.T.dot(p-alpha)/len(y)
        w=w0-eta*der
        al=abs(np.asarray(w0-w))
        if al.all()<1e-8:
            break        
    p=softmax(w,X)
    y_pred=p.idxmax(axis=1)
    y_pred=le.inverse_transform(y_pred)
    print(iter0)
    return y_pred,w

def gradDesAdap(X,y,w0,eta,iter0):
    loss=get_loss(w0,X,y)
    p=softmax(w0,X)
    w=w0
    alpha=np.asarray(pd.get_dummies(y))
    eta0=eta
    for i in range(iter0):
        w0=w
        p=softmax(w,X)
        der=X.T.dot(p-alpha)/len(y)
        eta=eta0/np.sqrt(i+1)
        w=w0-eta*der
    p=softmax(w,X)
    y_pred=p.idxmax(axis=1)
    y_pred=le.inverse_transform(y_pred)
    print(iter0)
    return y_pred,w

def gradDesBack(X,y,w0,eta,iter0,alp,beta):

#Source: https://www.cs.cmu.edu/~ggordon/10725-F12/scribes/10725_Lecture5.pdf
    
    loss=get_loss(w0,X,y)
    p=softmax(w0,X)
    w=w0
    alpha=np.asarray(pd.get_dummies(y))
    eta=alp
    for i in range(iter0):
        p=softmax(w,X)
        der=alp*(-1/len(y))*X.T.dot(alpha-p)
        while(get_loss(w-eta*der,X,y)>get_loss(w,X,y)-alp*eta*(np.linalg.norm(der)**2)):    
            eta=eta*beta
        w=w-eta*der
    print(iter0)
    p=softmax(w,X)
    y_pred=p.idxmax(axis=1)
    y_pred=le.inverse_transform(y_pred)
    return y_pred,w

f=open(sys.argv[3],'r')
s=f.read()
a=s.split()
n=int(a[0])

if n ==1:
    params = pd.read_csv(sys.argv[3],header=None)
    eta=params.iloc[1,0]
    iter0=int(params.iloc[2,0])
    y_pred,w=gradDes(X,y,w0,eta,iter0)
    p=softmax(w,X_test)
    y_pred= p.idxmax(axis=1)
    y_pred=le.inverse_transform(y_pred)    
    np.savetxt(sys.argv[4],y_pred,delimiter=" ", fmt="%s")
    w=w[[0,2,4,1,3]]
    np.savetxt(sys.argv[5],w,delimiter=",")    
    
elif (n) ==2:
    params = pd.read_csv(sys.argv[3],header=None)
    eta=params.iloc[1,0]
    iter0=int(params.iloc[2,0])
    y_pred,w=gradDesAdap(X,y,w0,eta,iter0)
    p=softmax(w,X_test)
    y_pred= p.idxmax(axis=1)
    y_pred=le.inverse_transform(y_pred)    
    np.savetxt(sys.argv[4],y_pred,delimiter=" ", fmt="%s")
    w=w[[0,2,4,1,3]]
    np.savetxt(sys.argv[5],w,delimiter=",")    

else:
    iter0=int(a[2])
    eta=float(a[1].split(',')[0])
    alp=float(a[1].split(',')[1])
    beta=float(a[1].split(',')[2])
    y_pred,w=gradDesBack(X,y,w0,0.1,iter0,0.5,0.1)
    p=softmax(w,X_test)
    y_pred= p.idxmax(axis=1)
    y_pred=le.inverse_transform(y_pred)    
    np.savetxt(sys.argv[4],y_pred,delimiter=" ", fmt="%s")
    w=w[[0,2,4,1,3]]
    np.savetxt(sys.argv[5],w,delimiter=",")    
